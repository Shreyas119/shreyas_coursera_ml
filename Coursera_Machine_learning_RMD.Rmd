---
title: "Coursera Machine Learning"
author: "Shreyas"
date: "August 6, 2016"
output: 
  html_document: 
    keep_md: yes
---
Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

Data

The training data for this project are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data are available here: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

Citation

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har].

Goal

The goal is to find a model that can predicht the classes below based on the sensor data of an activity.

exactly according to the specification (Class A)
throwing the elbows to the front (Class B)
lifting the dumbbell only halfway (Class C)
lowering the dumbbell only halfway (Class D)
throwing the hips to the front (Class E)

Loading and Cleaning Data

Once we load all packages we need to clean the data. We remove all coloums containing "NA" values and also the inital coloumns as they don't contribute to prediction

```{r}
library("dplyr")
library("caret")
library("tidyr")
library(rpart)
library(e1071)
set.seed(54356)
pml.training <- read.csv("pml-training.csv", na.strings = c("NA","#DIV/0!", ""), dec = ".")
pml.submission <- read.csv("~/pml-testing.csv")
x <- pml.training %>% filter(new_window == "no")
x <- x[8:length(x)]
x <- x[ , ! apply(x ,2 ,function(x) any(is.na(x)) ) ]
```

Creating training and testset for cross validation

We create a test and training set from the gven training set to build a model.
60% for trainig and rest for test.
```{r}
inTrain <- createDataPartition(y=x$classe,p=0.6, list=FALSE)
trainingset <- subset(x[inTrain,])
testset <- subset(x[-inTrain,])
```

Cross Validation
We'll be using traincontrol, instead of default option.

We do a varimp to find out the most impotant variables and then we built Decision trees and Random Forests.

Decision Trees
```{r}
Ctrl <- trainControl(method = "repeatedcv", repeats = 3)
model_rpart <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + roll_forearm + magnet_dumbbell_z + pitch_belt + magnet_dumbbell_y + magnet_dumbbell_x + accel_belt_z + magnet_belt_z, data=trainingset, method="rpart", tuneLength = 30, trControl = Ctrl)
model_rpart$finalModel
```

Random Forest model
```{r}
Ctrl <- trainControl(method = "oob")
model_rf <- train(classe ~ roll_belt + pitch_forearm + yaw_belt + roll_forearm + magnet_dumbbell_z + pitch_belt + magnet_dumbbell_y + magnet_dumbbell_x + accel_belt_z + magnet_belt_z, data=trainingset, method="rf", trControl = Ctrl, tuneGrid = data.frame(.mtry = 2))
model_rf
```

We now build a confusion matrix to find out which is the better model
For Decision Trees

```{r}
predictions_rparttest <- predict(model_rpart, testset)
confusionMatrix(predictions_rparttest, testset$classe)[3]
```

```{r}
conf_rpart <- as.data.frame(confusionMatrix(predictions_rparttest, testset$classe)[2])
conf_rpart <- conf_rpart %>% rename(prediction = table.Prediction, reference = table.Reference, count = table.Freq) %>% 
  arrange(desc(prediction)) %>% group_by(prediction) %>% mutate(prob = count/sum(count)) %>% ungroup
ggplot(conf_rpart, aes(reference, prediction)) + 
  geom_tile(aes(fill = prob), colour = "white") + 
  geom_text(aes(fill = prob, label = round(prob, 2)), size=3, colour="grey25") +
  scale_fill_gradient(low = "white", high = "red") +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0), limits = c("E","D","C","B","A")) 
```

For Random Forests

```{r}
predictions_rftest <- predict(model_rf, testset)
confusionMatrix(predictions_rftest, testset$classe)[3]
```

```{r}
conf_rf <- as.data.frame(confusionMatrix(predictions_rftest, testset$classe)[2])
conf_rf <- conf_rf %>% rename(prediction = table.Prediction, reference = table.Reference, count = table.Freq) %>% 
  arrange(desc(prediction)) %>% group_by(prediction) %>% mutate(prob = count/sum(count)) %>% ungroup
ggplot(conf_rf, aes(reference, prediction)) + 
  geom_tile(aes(fill = prob), colour = "white") + 
  geom_text(aes(fill = prob, label = round(prob, 2)), size=3, colour="grey25") +
  scale_fill_gradient(low = "white", high = "red") +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0), limits = c("E","D","C","B","A")) 
```

Now that we have build the consusion matrix we can decide which is the better model.

Now we'll find out the Sample error rate of Random Forest

```{r}
model_rf$finalModel
confusionMatrix(predictions_rftest, testset$classe)[3]
```

The in sample error of the random forest model is 1.69%. The out of sample error is 1.71% (1 - out of sample accuracy).

Prediction using Random Forest

```{r}
bestfit <- model_rf
predprob <- predict(bestfit, pml.submission, type = "prob")
predprob$testcase <- 1:nrow(predprob)
predprob <- gather(predprob, "class", "prob", 1:5)
ggplot(predprob, aes(testcase, class)) + 
  geom_tile(aes(fill = prob), colour = "white") + 
  geom_text(aes(fill = prob, label = round(prob, 2)), size=3, colour="grey25") +
  scale_fill_gradient(low = "white", high = "red") +
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) 
final_predictions <- predict(bestfit, pml.submission)
final_predictions
```

Now we could genertate the file

```{r}
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(final_predictions)
```